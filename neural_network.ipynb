{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Our dataset is categorized by winner data and loser data. However, in the prediction stage, we don't want to leak who the winner is to the model. To avoid this issue, we will change the winners and losers to Player A and Player B, with a label of 1 (winner) or 0 (loser). This way, the features will not already indicate who won and who didn't.\n",
    "\n",
    "We created a regular pairing and flipped pairing (Player A is winner vs. Player A is loser) to make the model symmetric and double our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(path, compression=\"gzip\") # full dataset\n",
    "\n",
    "df = []\n",
    "\n",
    "f = gzip.open(\"cse158-assignment2-master.csv.gz\")\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    df.append(d)\n",
    "    if len(df) >= 20000: # what's a good number?\n",
    "        break\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_columns = [\n",
    "        \"trophies\", \"isinclan\", \"cards\", \"troops\", \"structures\", \"spells\",\n",
    "        \"commons\", \"rares\", \"epics\", \"legendaries\", \"elixir.average\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A is winner, B is loser\n",
    "rows_A_winner = {}\n",
    "for stat in stat_columns:\n",
    "    rows_A_winner[f\"A.{stat}\"] = df[f\"winner.{stat}\"]\n",
    "    rows_A_winner[f\"B.{stat}\"] = df[f\"loser.{stat}\"]\n",
    "y_A_winner = pd.Series(np.ones(len(df)), name=\"label\")   # A won = 1\n",
    "\n",
    "df_A = pd.DataFrame(rows_A_winner)\n",
    "df_A[\"label\"] = y_A_winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B is winner, A is loser\n",
    "rows_B_winner = {}\n",
    "for stat in stat_columns:\n",
    "    rows_B_winner[f\"A.{stat}\"] = df[f\"winner.{stat}\"]\n",
    "    rows_B_winner[f\"B.{stat}\"] = df[f\"loser.{stat}\"]\n",
    "y_B_winner = pd.Series(np.zeros(len(df)), name=\"label\")   # A lost = 0\n",
    "\n",
    "df_B = pd.DataFrame(rows_B_winner)\n",
    "df_B[\"label\"] = y_B_winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs = pd.concat([df_A, df_B], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Feature Vector\n",
    "\n",
    "We used difference features, like delta_trophies (A.trophies - B.trophies), to improve prediction accuracy for competitive game matchups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_features = {}\n",
    "\n",
    "for stat in stat_columns:\n",
    "    diff_features[f\"delta_{stat}\"] = df_pairs[f\"A.{stat}\"] - df_pairs[f\"B.{stat}\"]\n",
    "# are absolute features like total cards needed?\n",
    "\n",
    "df_diff = pd.DataFrame(diff_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Matrix\n",
    "\n",
    "X = df_diff\n",
    "y = df_pairs[\"label\"]\n",
    "\n",
    "return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Network\n",
    "\n",
    "using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClashRoyaleNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClashRoyaleNetwork(input_dim=X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    predictions = model(torch.tensor(X_train, dtype=torch.float32))\n",
    "    loss = criterion(predictions.squeeze(), torch.tensor(y_train, dtype=torch.float32))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch {epoch} | loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(X_test, dtype=torch.float32)).squeeze()\n",
    "    predictions = (predictions > 0.5).numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
